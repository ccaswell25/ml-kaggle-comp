# Overview:

The final lab for our Machine Learning course will be to train models to predict dissolved inorganic carbon in water samples collected by the California Cooperative Oceanic Fisheries Investigations program (CalCOFI). This is a part of a class-wide Kaggle competition, completed on March 23rd.

This data will be used to train a model that will predict dissolved inorganic carbon (DIC) content in water samples from CalCOFI.

Details on the database are located here: <https://calcofi.org/data/oceanographic-data/bottle-database/>

## Load Libraries:

```{r warning = FALSE, message = FALSE}
library(tidymodels)
library(dplyr)
library(here)
library(tidyverse)
library(kernlab)
```

## Read in the Data:

```{r warning = FALSE, message = FALSE}
#annie's
dic_train <- read_csv(here("train.csv")) %>% select(-...13)
dic_test <- read_csv(here("test.csv")) %>% drop_na()

```

## Explore the Data:

```{r warning = FALSE, message = FALSE}
# Data Exploration ---
head(dic_train)

glimpse(dic_train)
```

```{r}
# Plotting to visualize data ---

ggplot(data = dic_train, aes(x = Lon_Dec, y = Lat_Dec, color = DIC)) + # initiate ggplot and add variables
  geom_point() + # add scatter points
  scale_color_gradient(low = "blue", high = "red") + # create end of color gradient
  labs(title = "Dissolved Inorganic Carbon (DIC) Visualization",
       x = "Longitude", # x axis label
       y = "Latitude", # y axis label
       color = "DIC") + # legend label
  theme_minimal()

```





## Choose Model Algorithm:

We chose Random Forest because it achieved results with fairly low computational time, while not undermining results. 



# Random Forest

## Pre-Processing:

```{r  RF pre-processing, warning = FALSE, message = FALSE}
# Create the recipe ---
dic_rec <- recipe(DIC ~ ., data = dic_train) %>% 
  step_center(all_numeric_predictors()) %>%  # center to mean of 0
  step_scale(all_numeric_predictors()) # scale to sd of 1
  
# Preprocessing ---
dic_model <- rand_forest(mtry = tune(), trees = tune()) %>% #specify random forest as model
  set_engine("randomForest") %>% #random forest as engine
  set_mode("regression") # regression as opposed to classification since we are trying to predict dic

# Specifying Workflow ---
dic_workflow = workflow() %>% 
  add_model(dic_model) %>% 
  add_recipe(dic_rec)
```


## Tune Relevant Parameters (Cross validation):

```{r Rf tuning, warning = FALSE, message = FALSE}
#Creating folds for cross validation ---
set.seed(1234)
folds <-vfold_cv(dic_train, strata = DIC) # create folds for cross validation


#Tune the Model ---
fit_dic <- dic_workflow %>%  
tune_grid(
  resamples = folds, # resample with cross validation
 grid = 10) # set grid = 10 for tuning

collect_metrics(fit_dic) # look at results 



```

```{r RF plot best fit, warning = FALSE, message = FALSE}
# Visualizing results ---
autoplot(fit_dic) # examine how different parameter configurations relate to accuracy 
show_best(fit_dic) # show top models and performance
```



## Final Predictions:

```{r random forest predictions , warning = FALSE, message = FALSE}
# Final Predictions ---
# Finalizing workflow with best metrics ---
best_model_params = dic_workflow %>% 
  finalize_workflow(select_best(fit_dic, metric = "rmse")) # finalize workflow based on best rmse

# Fit the model to the training set ---
 fit_model = fit(best_model_params, dic_train) # fit model

# Test predictions on model ---
predict_model = predict(fit_model, dic_test) %>%  # predict on test data
  bind_cols(dic_test) # add predict column to dic_test

# Store accuracy of testing prediction ---
accuracy <- accuracy(predict_model, truth = DIC, estimate = .pred_class) # get accuracy from ground truth 

print(paste0("I get an accuracy measure of ", round(accuracy[,3], 2), " with this model."))

```



